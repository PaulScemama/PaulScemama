[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Paul Scemama. I am a Machine Learning Engineer at MITRE. I’m interested in scalable uncertainty quantification, Bayesian experimental design, and program design."
  },
  {
    "objectID": "posts/curve_fitting/index.html",
    "href": "posts/curve_fitting/index.html",
    "title": "Approaches to Curve Fitting",
    "section": "",
    "text": "This post aims to introduce different approaches to curve fitting, and in particular, different parametric approaches to curve fitting. It follows closely section 1.2.5 from Bishop’s Pattern Recognition and Machine Learning (Bishop and Nasrabadi (2006)), where I expand on some of the more vague concepts introduced. Additionally, (Tipping (2003)) is a good resource and has some nicer visualizations regarding the sampling of priors and posteriors. The goal is to see how incorporating particular assumptions into the probabilistic model leads to instances of deterministic approaches. Finally, the Appendix contains some extra derivations and code to “actually do” the curve fitting."
  },
  {
    "objectID": "posts/curve_fitting/index.html#the-model",
    "href": "posts/curve_fitting/index.html#the-model",
    "title": "Approaches to Curve Fitting",
    "section": "2.1 The Model",
    "text": "2.1 The Model\nIn the deterministic approach, we simply consider our target variable \\(t\\) to be a parameterized function of (i) the input variable \\(x\\) and (ii) some unknown parameters \\(\\mathbf{w}\\). In other words, we assume \\(t\\) is the result of a non-random data-generating process which can be described by a function within the family of functions\n\\[\ny(x, \\mathbf{w}) = w_0 + w_1x + w_2x^2 + \\dots + w_Mx^M = \\sum_{j=0}^M w_j x^j, \\tag{2.1}\n\\]\nwhere \\(M\\) is the order of the polynomial, and the polynomial coefficients \\(w_0, ..., w_M\\) are packaged in the vector \\(\\mathbf{w}\\). Although \\(y(x, \\mathbf{w})\\) is a nonlinear function of \\(x\\), it is linear in the unknown parameters \\(\\mathbf{w}\\). Functions that are linear in the unknown parameters have special properties and are called linear models."
  },
  {
    "objectID": "posts/curve_fitting/index.html#least-squares-estimation",
    "href": "posts/curve_fitting/index.html#least-squares-estimation",
    "title": "Approaches to Curve Fitting",
    "section": "2.2 Least-Squares Estimation",
    "text": "2.2 Least-Squares Estimation\nAlthough we’re trying to model the variable \\(t\\) as a function of the input variable \\(x\\), we only have realizations of these variables which we’ve stored in \\(\\{\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}\\}\\). In the deterministic approach we try and find a setting of \\(\\mathbf{w}\\) that best agrees with our dataset \\(\\{\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}\\}\\) by minimizing some notion of “error”. This error measures the misfit between a realized model – the function \\(y(x, \\mathbf{w})\\) for a given setting of \\(\\mathbf{w}\\) – and the training data points. A widely used (and simple) error function is given by the sum-of-squares of the Euclidean distance between predictions \\(y(x_n, \\mathbf{w})\\) and the corresponding target values \\(t_n\\),\n\\[\nE(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\}^2. \\tag{2.2}\n\\]\nThe value of \\(E(\\mathbf{w})\\) is always non-negative and is zero only when \\(y(x, \\mathbf{w})\\) passes exactly through every training point. Using the error function \\((2.2)\\) in order to find the parameter values for \\(\\mathbf{w}\\) is called the method of least-squares. It is visualized in Figure 1.\n\n\n\nFigure 1: The sum-of-squares error function in \\((3)\\) is computed by taking one half the sum of the squared distances of each data point from the function \\(y(x, \\mathbf{w})\\). These displacements are shown in red.\n\n\nSolving for \\(\\mathbf{w}\\) in this setting is fairly straightforward. Because the error function \\((2.2)\\) is a quadratic function of the parameters \\(\\mathbf{w}\\), its derivative with respect to \\(\\mathbf{w}\\) will be linear in the elements of \\(\\mathbf{w}\\). Therefore, the minimization of the error function has a unique solution, which we denote \\(\\mathbf{w}^*\\). It can be found in closed form (Appendix 1). We can then use \\(y(x, \\mathbf{w}^{*})\\) to predict new values of the target \\(t\\) for new observed values of the input \\(x\\)."
  },
  {
    "objectID": "posts/curve_fitting/index.html#overfitting",
    "href": "posts/curve_fitting/index.html#overfitting",
    "title": "Approaches to Curve Fitting",
    "section": "2.3 Overfitting",
    "text": "2.3 Overfitting\nThere remains the need to choose the order \\(M\\) of the polynomial, which falls within the realm of model selection. Choosing a large \\(M\\) yields a flexible set of models for us to fit, but they are susceptible to overfitting. We will see later that the least-squares method represents a special case of maximum likelihood, and that overfitting can be viewed as a general symptom of maximum likelihood.\nFor now, we can continue with a particular method to avoid overfitting – regularization. This involves adding a new term to the error function \\((2.2)\\) that penalizes the parameters \\(\\mathbf{w}\\) for being too large. The simplest penalty term to add is the sum-of-squares of the weights. This leads to a new error function,\n\\[\n\\overset{\\sim}{E} (\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) \\}^2 + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2 \\tag{2.3},\n\\]\nwhere \\(\\lVert \\mathbf{w} \\rVert^2 = \\mathbf{w}^\\intercal \\mathbf{w} = w_0^2 + w_1^2 + \\dots + w_M^2\\) and the parameter \\(\\lambda\\) controls the strength of regularization. Like \\((2.2)\\), the error function \\((2.3)\\) can be minimized in close form (Appendix 2). Instituting such a penalty term as we did takes on different names depending on the literature. In the statistics literature they are known as shrinkage methods, and in the context of neural networks it is known as weight decay. Lastly, the specific case of \\((2.3)\\) is known as ridge regression."
  },
  {
    "objectID": "posts/curve_fitting/index.html#the-model-1",
    "href": "posts/curve_fitting/index.html#the-model-1",
    "title": "Approaches to Curve Fitting",
    "section": "3.1 The Model",
    "text": "3.1 The Model\nIn the deterministic approach we assumed \\(t\\) to be the result of a deterministic function of \\(x\\) and unknown parameters \\(\\mathbf{w}\\). We now consider a probabilistic model so that we can express uncertainty in our predictions.\nWe may not want to make such a strong statement as saying “\\(t\\) is exactly equal to \\(y(x, \\mathbf{w})\\)” as we did in the previous section. This could be because we think there is noise in the observations \\(\\pmb{\\mathsf{t}}\\), for example due to measurement error. To articulate this assumed reality we need to place a distribution over the target variable \\(t\\). A sensible distributional assumption is to place a Gaussian distribution over \\(t\\) with its mean given by the parameterized function \\(y(x, \\mathbf{w})\\) and its variance being fixed and unknown. This is visualized in Figure 2.\n\n\n\nFigure 2: Illustration of a Gaussian conditional distribution over \\(t\\) conditioned on \\(x\\) where the mean of the distribution is given by some function of \\(x\\), and the variance is fixed\n\n\nTo understand what we’re effectively saying when we create such a model, it is useful to re-emphasize and apply the data-generating process perspective. We can think of our model as describing a process that produces \\(t\\) from a given \\(x\\) and parameter setting \\(\\mathbf{w}\\). Last section, this process was a deterministic function \\(y(x, \\mathbf{w})\\). In this section, we extend the process by positing that each \\(t\\) is the result of \\(y(x, \\mathbf{w})\\) and some additive uncertainty, where that additive uncertainty takes the form of a zero-mean Gaussian distribution with unknown variance. This is to say that we are assuming, to have gotten a particular instance of \\(t\\):\n\nwe are given an instance of \\(x\\),\nthis instance of \\(x\\) is then used to obtain the output of the parameterized function \\(y(x, \\mathbf{w})\\),\nto which we add a sample from a zero-mean Gaussian with fixed and unknown variance.\n\nThis leads to the following model,\n\\[\\begin{align}\np(t|x, \\mathbf{w}, \\beta) &= y(x, \\mathbf{w}) + \\mathcal{N}(0, \\beta^{-1}) \\notag \\\\\n&= \\mathcal{N}(y(x, \\mathbf{w}), \\beta^{-1}), \\tag{3.1}\n\\end{align}\\]\nwhere we’ve used the scaling property of the Gaussian distribution’s mean. \\((3.1)\\) is an observation model and is more specifically referred to as the Gaussian noise model or a conditional Gaussian model."
  },
  {
    "objectID": "posts/curve_fitting/index.html#maximum-likelihood-estimation",
    "href": "posts/curve_fitting/index.html#maximum-likelihood-estimation",
    "title": "Approaches to Curve Fitting",
    "section": "3.2 Maximum Likelihood Estimation",
    "text": "3.2 Maximum Likelihood Estimation\nIn order to use the training dataset \\(\\{\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}\\}\\) to determine the values of the unknown parameters \\(\\mathbf{w}\\) and \\(\\beta\\), we will use a more general approach than error minimization – maximum likelihood estimation. As the name suggests, we will search for a setting of \\(\\mathbf{w}\\) and \\(\\beta\\) so that the likelihood of our observed data \\(\\pmb{\\mathsf{t}}\\) is maximized. In other words, we’ve defined a data-generating process, and we want to find the setting of the parameters such that the probabilities of our process having created each observed \\(t_n \\in \\pmb{\\mathsf{t}}\\) from each \\(x_n \\in \\pmb{\\mathsf{x}}\\) are maximized. The likelihood measures the aggregation of all these point-wise probabilities.\nIn order to use maximum likelihood estimation, we need to have a likelihood function. A likelihood function is derived from an observation model. It can be thought of as an observation model being applied to a particular dataset. Assuming the data \\(\\pmb{\\mathsf{t}}\\) were independently sampled from \\((3.1)\\), the likelihood function is the product of evaluating how consistent the model is with each datapoint \\((t_n, x_n)\\), and is evaluated for a particular setting of \\(\\mathbf{w}\\) and \\(\\beta\\),\n\\[\np(\\pmb{\\mathsf{t}}| \\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta) = \\prod_{n=1}^N \\mathcal{N}(t_n|y(x_n, \\mathbf{w}), \\beta^{-1}). \\tag{3.2}\n\\]\nEach time we choose a setting for \\(\\mathbf{w}\\) and \\(\\beta\\) and plug them into our model, we are defining a conditional distribution given by \\((3.1)\\). This conditional distribution may agree with the dataset we have, or it may not. Examples of agreement and disagreement are shown in Figure 3.\n\n\n\nFigure 3: A Gaussian noise model shown for a handful of \\(x_i\\), with two different settings for \\(\\mathbf{w}\\) and \\(\\beta\\). On the left is a setting of \\((\\mathbf{w}, \\beta)\\) that induces a model that disagrees with our observed data. On the right is a setting of \\((\\mathbf{w}, \\beta)\\) that induces a model that agrees much better with our observed data. Maximum likelihood looks for a setting of \\((\\mathbf{w}, beta)\\) that best agrees with our observed data.\n\n\nWe now demonstrate how, in practice, we compute the maximum likelihood estimates for \\(\\mathbf{w}\\) and \\(\\beta\\). In this example, it can be done in closed form and amounts to taking the derivative of the likelihood \\((3.2)\\), setting it equal to zero, and then solving for \\(\\mathbf{w}\\) or \\(\\beta\\). We begin with \\(\\mathbf{w}\\). It is common to instead maximize the log likelihood instead of the likelihood \\((3.2)\\) for numerical stability and convenience. We can write the log likelihood as\n\\[\n\\text{ln}p(\\pmb{\\mathsf{t}}| \\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta) = - \\frac{\\beta}{2} \\sum_{n=1}^N \\{y(x_n ,\\mathbf{w}) - t_n \\}^2 + \\frac{N}{2} \\text{ln}\\beta - \\frac{N}{2} \\text{ln}(2 \\pi). \\tag{3.3}\n\\]\n\n3.2.1 Maximum Likelihood’s connection to Least-Squares\nIn taking the derivative of \\((3.3)\\) with respect to \\(\\mathbf{w}\\), we can omit the last two terms as they do not depend on \\(\\mathbf{w}\\). We can also replace the coefficient \\(\\frac{\\beta}{2}\\) with \\(\\frac{1}{2}\\) since scaling \\((3.3)\\) by a constant won’t change the location of the maximum of \\((3.3)\\) with respect to \\(\\mathbf{w}\\). Lastly, we can equivalently minimize the negative log likelihood. This leaves us with minimizing\n\\[\n\\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\}^2. \\tag{3.4}\n\\]\nAnd so we see that the sum-of-squares error function has arisen as a consequence of maximizing the likelihood under the assumption of a Gaussian noise distribution. In fact, for a Gaussian noise model, maximum likelihood estimation and least-squares estimation find the same \\(\\mathbf{w}\\); in particular, the one that minimizes \\((3.4)\\). Once we’ve found the maximum likelihood estimate for \\(\\mathbf{w}\\), which we will denote \\(\\mathbf{w}_{\\text{ML}}\\), we can use it to find the setting for the precision parameter \\(\\beta\\) of the Gaussian conditional distribution. Maximizing \\((3.3)\\) with respect to \\(\\beta\\) yields\n\\[\n\\frac{1}{\\beta_{\\text{ML}}} = \\frac{1}{N} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}_{\\text{ML}}) - t_n \\}^2. \\tag{3.5}\n\\]\nAnd so we see that the maximum likelihood procedure yields a variance \\(\\sigma^2\\) as equalling the average squared deviation between the observed data points and the function \\(y(x, \\mathbf{w}_{\\text{ML}})\\).\n\n\n3.2.2 Maximum Likelihood’s predictive distribution\nThe predictive distribution as a result of the maximum likelihood approach amounts to plugging in the maximum likelihood estimates \\(\\mathbf{w}_{\\text{ML}}\\) and \\(\\beta_{\\text{ML}}\\) into the observation model 3.1:\n\\[\np(t|x, \\mathbf{w}_{\\text{ML}}, \\beta_{\\text{ML}}) = \\mathcal{N}(y(x, \\mathbf{w}_{\\text{ML}}), \\beta^{-1}_{\\text{ML}}). \\tag{3.6}\n\\]"
  },
  {
    "objectID": "posts/curve_fitting/index.html#maximum-a-posteriori-estimation",
    "href": "posts/curve_fitting/index.html#maximum-a-posteriori-estimation",
    "title": "Approaches to Curve Fitting",
    "section": "3.3 Maximum a posteriori Estimation",
    "text": "3.3 Maximum a posteriori Estimation\nIntroducing a prior distribution over the parameters \\(\\mathbf{w}\\) is a way of introducing our prior beliefs (perhaps through domain expertise) about the parameters before observing our dataset. Additionally, as we will see, it serves as a regularizer for our estimate of \\(\\mathbf{w}\\). Importantly, it is also one of the components in Bayes’ theorem, and takes us a step towards a full Bayesian treatment. For simplicity, we introduce a simple Gaussian prior\n\\[\np(\\mathbf{w}| \\alpha) = \\mathcal{N}(\\mathbf{w} | \\mathbf{0}, \\alpha^{-1} \\mathbf{I}) = \\left( \\frac{\\alpha}{2 \\pi} \\right)^{(M+1) / 2} \\text{exp} \\left[- \\frac{\\alpha}{2} \\mathbf{w}^{\\intercal} \\mathbf{w} \\right], \\tag{3.7}\n\\]\nwhere \\(\\alpha\\) is the precision of the distribution and \\(M+1\\) is the number of elements in \\(\\mathbf{w}\\) for an \\(M\\) order polynomial function. Variables such as \\(\\alpha\\) are called hyperparameters since we have to choose their values. Now that we have a prior, we can use Bayes’ theorem to yield a quantity proportional to the posterior,\n\\[\n\\overbrace{ p(\\mathbf{w}|\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}, \\alpha, \\beta) }^{\\text{Posterior } p(\\mathbf{w}|D)} = \\frac{\\overbrace{p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta)}^{\\text{Likelihood } p(D|\\mathbf{w})} \\; \\cdot \\; \\overbrace{p(\\mathbf{w}|\\alpha)}^{\\text{Prior }p(\\mathbf{w})}}{\\underbrace{p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}})}_{\\text{Model Evidence } p(D)}} \\propto \\overbrace{p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta)}^{\\text{Likelihood } p(D|\\mathbf{w})} \\; \\cdot \\; \\overbrace{p(\\mathbf{w}|\\alpha)}^{\\text{Prior }p(\\mathbf{w})}, \\tag{3.8}\n\\]\nwhere the proportion relation \\(\\propto\\) comes from the fact that the denominator of Bayes’ theorem, \\(p(D) = p(\\mathbf{t} | \\mathbf{x})\\), does not depend on \\(\\mathbf{w}\\). Maximizing the right hand size of \\((3.8)\\) with respect to \\(\\mathbf{w}\\) is equivalent to maximizing the posterior with respect to \\(\\mathbf{w}\\) due to the proportionality. By optimizing \\(\\mathbf{w}\\) to maximize the posterior, we are finding the most probable parameter values \\(\\mathbf{w}\\) considering our observed data and also our prior knowledge about \\(\\mathbf{w}\\) encapsulated in the (data independent) prior \\(p(\\mathbf{w} | \\alpha)\\). This yields a tradeoff between what we believed about \\(\\mathbf{w}\\) before seeing our data, and the \\(\\mathbf{w}\\) that best fits our data. This technique is referred to maximum a posteriori estimation, MAPE, or MAP.\n\n3.3.1 Maximum a posteriori’s connection to Least-Squares\nTaking the negative logarithm of \\((3.8)\\) and combining with the log likelihood in \\((3.3)\\) and the prior in \\((3.7)\\), it can be shown that the maximum of the posterior is equivalenty the minimum of the following expression:\n\\[\n\\frac{\\beta}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\}^2 + \\frac{\\alpha}{2} \\mathbf{w}^\\intercal \\mathbf{w}. \\tag{3.9}\n\\]\nIf we define \\(\\lambda = \\alpha / \\beta\\), we can rewrite \\((3.9)\\) as\n\\[\n\\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\}^2 + \\frac{\\lambda}{2} \\mathbf{w}^\\intercal \\mathbf{w}. \\tag{3.10}\n\\]\nAnd so we see that minimizing the regularized sum-of-squares function introduced in \\((4)\\) arises naturally from maximizing the posterior of a Gaussian noise model with Gaussian prior.\nSimilar to the maximum likelihood section, once we’ve found the maximum a posteriori estimate for \\(\\mathbf{w}\\), which we will denote \\(\\mathbf{w}_{\\text{MAP}}\\), we can use it to find the setting for the precision parameter \\(\\beta\\). We did not introduce a prior over \\(\\beta\\), so the negative logarithm of the right-hand-side of \\((3.8)\\) is\n\\[\n-\\text{ln} \\left[ \\: p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta) p(\\mathbf{w}|\\alpha)  \\: \\right] = -[ \\: \\text{ln}p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta) + \\text{ln} p(\\mathbf{w}|\\alpha) \\: ]. \\tag{3.11}\n\\]\nSince we’ve only gained an additive term that does not functionally depend on \\(\\beta\\), solving for \\(\\beta_{\\text{MAP}}\\) yields a near-equivalent expression to \\((3.5)\\) except we now use \\(\\mathbf{w}_{\\text{MAP}}\\) instead of \\(\\mathbf{w}_{\\text{ML}}\\):\n\\[\n\\frac{1}{\\beta_{\\text{MAP}}} = \\frac{1}{N} \\sum_{n=1}^{N} \\{y(x_n, \\mathbf{w}_{\\text{MAP}}) - t_n\\}^2. \\tag{3.12}\n\\]\n\n\n3.3.2 Maximum a posteriori’s predictive distribution\nSimilar to Maximum Likelihood, the result of the maximum a posteriori estimation method are point-estimates for the parameters \\(\\mathbf{w}\\) and \\(\\beta\\). We can similarly plug in those estimates, denoted \\(\\mathbf{w}_{\\text{MAP}}\\) and \\(\\beta_{\\text{MAP}}\\), into the observation model \\((3.1)\\):\n\\[\np(t|x, \\mathbf{w}_{\\text{MAP}}, \\beta_{\\text{MAP}}) = \\mathcal{N}(y(x, \\mathbf{w}_{\\text{MAP}}), \\beta^{-1}_{\\text{MAP}}). \\tag{3.13}\n\\]"
  },
  {
    "objectID": "posts/curve_fitting/index.html#bayesian-estimation",
    "href": "posts/curve_fitting/index.html#bayesian-estimation",
    "title": "Approaches to Curve Fitting",
    "section": "3.4 Bayesian Estimation",
    "text": "3.4 Bayesian Estimation\nSo far we have been making a point estimate of \\(\\mathbf{w}\\) which does not yet amount to a Bayesin treatment. In a Bayesian treatment, we take into account all possible \\(\\mathbf{w}\\) that could have explained our data. To predict a new value of \\(t\\) for a new \\(x\\), we marginalize over all possible settings of \\(\\mathbf{w}\\), yielding the posterior predictive distribution or Bayesian model average. The goal of the Bayesian treatment is to compute the posterior over the weights \\(p(\\mathbf{w}|D)\\) which represents all possible settings of \\(\\mathbf{w}\\) that give rise to models that can explain our observed data \\(D\\). To this end, we must use Bayes’ theorem:\n\\[\np(\\mathbf{w}|D) = \\frac{p(D|\\mathbf{w})p(\\mathbf{w})}{p(D)}. \\tag{3.14}\n\\]\nAnd for our regression problem in particular, \\((3.14)\\) can be further specified as\n\\[\np(\\mathbf{w}|\\pmb{\\mathsf{t}}, \\pmb{\\mathsf{x}}, \\alpha, \\beta) = \\frac{\\overbrace{p(\\pmb{\\mathsf{t}} | \\pmb{\\mathsf{x}}, \\mathbf{w}, \\beta)}^{\\text{Likelihood}} \\cdot \\overbrace{p(\\mathbf{w} | \\alpha)}^{\\text{Prior}}}{\\underbrace{p(\\pmb{\\mathsf{t}} | \\pmb{\\mathsf{x}})}_{\\text{Evidence}}}.\n\\tag{3.15}\n\\]\nTo simplify this, we will assume the parameters \\(\\alpha\\) and \\(\\beta\\) are fixed and known in advance, and we will solely focus on the unknown \\(\\mathbf{w}\\). \\((3.15)\\) then turns into\n\\[\np(\\mathbf{w} \\vert \\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}) = \\frac{p(\\pmb{\\mathsf{t}} \\vert \\pmb{\\mathsf{x}}, \\mathbf{w}) \\cdot p(\\mathbf{w})}{p(\\pmb{\\mathsf{t}} \\vert \\pmb{\\mathsf{x}})}. \\tag{3.16}\n\\]\nSo instead of computing a single point estimate of the weights (as we’ve done thus far); now, given the likelihood and the prior, we can compute the posterior distribution over \\(\\mathbf{w}\\) via Bayes’ rule. Computing the posterior can be facilitated by our choice of prior (a modeling choice) so that we can calculate the posterior in closed form. Those priors are referred to as conjugate priors. There are a number of other techniques used to instead approximate the posterior \\(p(\\mathbf{w}|\\pmb{\\mathsf{t}}, \\pmb{\\mathsf{x}})\\) when conjugacy is impossible. These include variational inference and markov chain monte carlo sampling. Pretty much all posterior approximation methods introduce different ways to circumvent the explicit calculation of the denominator \\(P(D) = p(\\pmb{\\mathsf{t}}|\\pmb{\\mathsf{x}})\\) which is intractable for any interesting model. We save all these methods for another post and assume we’ve found the exact posterior.\n\n3.4.1 Bayesian’s predictive distribution\nOnce it is found, the posterior represents all possible settings of \\(\\mathbf{w}\\) in that they induce models that can explain our data. To incorporate this information into a predictive distribution so that we can predict new values of an unobserved \\(t\\) given an unobserved \\(x\\), we marginalize over all possible settings of \\(\\mathbf{w}\\) like so:\n\\[\np(t|x, \\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}) = \\int p(t|x, \\mathbf{w}) p(\\mathbf{w}|\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}) \\text{d}\\mathbf{w}, \\tag{3.17}\n\\]\nwhere \\(p(t|x, \\mathbf{w})\\) is our model given by \\((3.1)\\) omitting the dependence on \\(\\beta\\), and \\(p(\\mathbf{w}|\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}})\\) is the posterior over the weights \\(\\mathbf{w}\\). In \\((3.15)\\),\n\nwe look at a possible setting of \\(\\mathbf{w}\\) which we will denote \\(\\mathbf{w}_i\\) according to the posterior \\(p(\\mathbf{w}|\\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}})\\). Placing \\(\\mathbf{w}_i\\) into our observation model \\(p(t|x, \\mathbf{w})\\) defines a “fitted model” \\(p(t|x, \\mathbf{w}_i)\\).\nthis “fitted model” is multiplied by the probability of that setting of \\(\\mathbf{w}_i\\) given by the posterior. In other words, the probability that \\(p(t|x, \\mathbf{w}_i)\\) was used to generate our observed data \\(\\pmb{\\mathsf{t}}\\) from the inputs \\(\\pmb{\\mathsf{x}}\\).\nwe do this for every possible setting \\(\\mathbf{w}_i\\) according to the posterior and integrate.\n\nThus, we are taking a weighted average of all possible “fitted models”, where the weights of each component are determined by how likely the setting of \\(\\mathbf{w}\\) is according to its posterior.\n\n\n3.4.2 Analyzing our specific predictive distribution\nWe will now analyze the specific form of \\((3.17)\\) for our example problem. A consequence of us selecting a Gaussian likelihood and a Gaussian prior is that we can analytically compute the posterior; and it is Gaussian as well – an example of conjugacy. More so, we can analytically solve the integration in \\((3.17)\\) to get a predictive distribution that itself is Gaussian. In fact, it takes on the more specific form\n\\[\np(t|x, \\pmb{\\mathsf{x}}, \\pmb{\\mathsf{t}}) = \\mathcal{N}(t|m(x), s^2(x)), \\tag{3.18}\n\\]\nwhere the mean and variance are given by\n\\[\\begin{align}\n&m(x) = \\beta \\pmb{\\phi}(x)^{\\intercal} \\mathbf{S} \\sum_{n=1}^N \\pmb{\\phi}(x_n) t_n \\tag{3.19} \\\\\n& s^2(x) = \\overbrace{\\beta^{-1}}^{\\text{noise}} + \\underbrace{\\pmb{\\phi}(x)^\\intercal \\mathbf{S} \\pmb{\\phi}(x)}_{\\text{Parameter Uncertainty}} \\tag{3.20}\n\\end{align}\\]\n\\(\\mathbf{S}\\) is a matrix, and is given by\n\\[\n\\mathbf{S}^{-1} = \\alpha \\mathbf{I} + \\beta \\sum_{n=1}^N \\pmb{\\phi}(x_n)  \\pmb{\\phi}(x),^\\intercal \\tag{3.21}\n\\]\nwhere \\(\\mathbf{I}\\) is the unit matrix, and we have defined the vector \\(\\pmb{\\phi}(x)\\) with elements \\(\\phi_i(x) = x^i\\) for \\(i=0,...,M\\). In looking at the predictive distribution \\((3.16)\\), we now see that the variance depends on \\(x\\). This is unlike the maximum likelihood predictive distribution \\((3.6)\\) and the maximum a posteriori predictive distribution \\((3.13)\\) where the variance is fixed for any value of \\(x\\). It’s expansion is described by \\((3.20)\\) and it contains two additive components. The first component, as was already expressed in the maximum likelihood predictive distribution \\(\\beta_{\\text{ML}}\\), is the noise on the target variables. The second component, which has not been expressed until now, arises from the uncertainty in the parameters \\(\\mathbf{w}\\) and is a consequence of treating \\(\\mathbf{w}\\) as a random variable – an artifact of the Bayesian treatment."
  },
  {
    "objectID": "posts/curve_fitting/index.html#solving-for-the-parameters",
    "href": "posts/curve_fitting/index.html#solving-for-the-parameters",
    "title": "Approaches to Curve Fitting",
    "section": "1 Solving for the parameters",
    "text": "1 Solving for the parameters\n\n1.1 Analytical Setup\nWe have the error function \\(E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\}^2\\), where \\(y(x, \\mathbf{w}) = \\sum_{j=0}^M w_j x^j\\). We’d like to find the optimal setting for \\(\\mathbf{w}\\) in the sense that it minimizes \\(E(\\mathbf{w})\\). We will see that we can cast this minimization problem as solving a system of linear equations. We will then solve that system of linear equations with code to attain the optimal \\(\\mathbf{w}^{*}\\).\nClaim: The \\(w_i\\) in \\(\\mathbf{w} = (w_0, w_1, w_2 ..., w_M)\\) that minimize the error function \\(E(\\mathbf{w})\\) are given by the solution to the following set of linear equations,\n\\[\n\\sum_{j=0}^M A_{ij}w_j = T_i \\;\\; \\text{ where } \\;\\; A_{ij} = \\sum_{n=1}^N(x_n)^{(i+j)}, \\; T_i = \\sum_{n=1}^N (x_n)^i t_n\n\\]\nProof: We will take the derivative of \\(E(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\), set it to zero, and then rearrange terms to prove the claim above.\nBy the chain rule,\n\\[\n\\frac{\\partial E(\\mathbf{w})}{\\partial w_i} = \\frac{\\partial E(\\mathbf{w})}{\\partial y(x_n, \\mathbf{w})} \\frac{\\partial y(x_n, \\mathbf{w})}{\\partial w_i} \\tag{1}\n\\]\nSolving the two terms on the right hand side yields\n\\[\n\\begin{aligned}\n&\\frac{\\partial E(\\mathbf{w})}{\\partial y(x_n, \\mathbf{w})} = \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\} \\\\\n&\\frac{\\partial y(x_n, \\mathbf{w})}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_0 + w_1x_1 + \\dots w_i x_n^i \\dots + w_m x_n^M) = x_n^i\n\\end{aligned}\n\\]\nSubstituting back into \\((1)\\) yields\n\\[\n\\begin{aligned}\n\\frac{\\partial E(\\mathbf{w})}{\\partial w_i} & = \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\} x_n^i \\\\\n& \\overset{(i)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{j} - t_n)x_n^i \\\\\n& \\overset{(ii)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{i}x_n^j - t_n x_n^i) \\\\\n& \\overset{(iii)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{(i+j)} - t_n x_n^i)\n\\end{aligned}\n\\]\nwhere in \\((\\text{i})\\) we use the definition of \\(y(x_n, \\mathbf{w})\\), in \\((\\text{ii})\\) we distribute \\(x_n^i\\) into the parentheses, and in \\((\\text{iii})\\) we use the exponent rule. Setting the derivative to \\(0\\) and rearranging,\n\\[\n\\begin{aligned}\n\\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{(i+j)} - t_n x_n^i) & = 0 \\\\\n\\sum_{n=1}^N \\sum_{j=0}^M w_j x_n^{(i+j)} & = \\sum_{n=1}^N t_n x_n^i \\\\\n\\sum_{j=0}^M A_{ij}w_j &= T_i\n\\end{aligned}\n\\]\n\\(\\blacksquare\\)\n\n\n1.2 Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Callable, List, Union, Tuple\n\n# ================ Generating Data ==================== #\ndef generate_data(\n    seed: int,\n    signal_fn: Callable,\n    noise: float,\n    num_samples: int,\n    range: List[float],\n    include_signal: bool = False,\n    plot: bool = False,\n) -&gt; Union[Tuple[Tuple[np.ndarray]], Tuple[np.ndarray]]:\n    \"\"\"\n    Generate noisy training data and the non-noisy data.\n\n    Parameters\n    ----------\n    seed : int\n        Seed for reproducibility.\n    signal_fn : Callable\n        A function describing the deterministic data generating process.\n    noise : float\n        The `scale` value for the Gaussian noise to be added.\n    num_samples : int\n        Number of training samples.\n    range : List[float]\n        Range of the input variable x.\n    plot : bool, optional\n        Whether to plot the data or not, by default False\n\n    Returns\n    -------\n    Union[Tuple[Tuple[np.ndarray]], Tuple[np.ndarray]]\n        If include_signal = True, two tuples where the first is the noisy\n        training data and the second is the underlying signal. If include_signal=False,\n        then a tuple with just the noisy training data.\n    \"\"\"\n    # Create generator for reproducible results\n    generator = np.random.default_rng(seed)\n    # Noisy signal\n    signal_noisy_fn = lambda xs: signal_fn(xs) + generator.normal(\n        loc=0, scale=noise, size=(num_samples,)\n    )\n    # Create training data\n    xs = np.linspace(start=range[0], stop=range[1], num=num_samples)\n    ts = signal_noisy_fn(xs)\n    if include_signal:\n        # Create \"signal\"\n        x_axis = np.linspace(start=0, stop=1, num=1000)\n        signal = signal_fn(x_axis)\n        to_return = ((xs, ts), (x_axis, signal))\n    else:\n        to_return = (xs, ts)\n    if plot:\n        # ===== Plot Data ====== #\n        # Create figure\n        figure1 = plt.figure(figsize=(8, 4))\n        plt.axis(\"off\")\n        # Plot Signal\n        line = np.linspace(start=0, stop=1, num=1000)\n        plt.plot(x_axis, signal, c=\"green\")\n        # Plot training data\n        plt.scatter(xs, ts, c=\"black\")\n        plt.show()\n    return to_return\n\n# ============= Solving for w ====================== #\ndef create_A(xs: np.array, M: int) -&gt; np.array:\n    \"\"\"\n    Create the matrix A where A_ij = sum_n=1^N [x_n^(i+j)].\n    \"\"\"\n    A = np.zeros((M, M))\n    for i in range(M):\n        for j in range(M):\n            A[i][j] = np.sum(xs ** (i + j))\n    return A\n\n\ndef create_T(xs: np.array, ts: np.array, M: int) -&gt; np.array:\n    \"\"\"\n    Create the vector T where T_i = sum_n=1^N (x_n^i) t_n.\n    \"\"\"\n    T = np.zeros((M,))\n    for i in range(M):\n        T[i] = np.sum((xs**i) * ts)\n    return T\n\n\ndef create_lambdaI(size: int, ln_lambda: float) -&gt; np.array:\n    \"\"\"\n    Create an identity matrix with lambda as the diagonal elements.\n    \"\"\"\n    lambda_ = np.exp(ln_lambda)\n    identity = np.eye(N=size)\n    return lambda_ * identity\n\n\ndef solve_for_w(\n    xs: np.array, ts: np.array, M: int, ln_lambda: float = None\n) -&gt; np.array:\n    \"\"\"\n    Creates A and T using `create_A` and `create_T` and then solves\n    the linear system of equations to get w.\n    \"\"\"\n    A = create_A(xs=xs, M=M)\n    T = create_T(xs=xs, ts=ts, M=M)\n    if ln_lambda:\n        lambdaI = create_lambdaI(size=M, ln_lambda=ln_lambda)\n        A_plus_lambdaI = A + lambdaI\n        return np.linalg.solve(a=A_plus_lambdaI, b=T)\n    else:\n        return np.linalg.solve(a=A, b=T)\n\n# ============== Prediction ====================== #\ndef predict(w: np.array, x: np.array, M) -&gt; np.array:\n    \"\"\"\n    Feature expand the input x and then run the linear\n    transformation involving w,x to get predictions for target t.\n    \"\"\"\n    # Get [x**1, x**2, x**3 ... x**M] for each x\n    N = x.shape[0]\n    powers = np.arange(0, M)\n    powers_expanded = np.tile(powers, (N,)).reshape(N, M)\n    xs_expanded = x.repeat(M).reshape(N, M)\n    xs_powered = np.power(xs_expanded, powers_expanded)\n    # Apply w\n    return w @ xs_powered.T\n\n# ========== Main =========== #\n(xs, ts), (x_axis, signal_values) = generate_data(\n    seed=123,\n    signal_fn=lambda x: np.sin(2 * np.pi * x),\n    noise=0.5,\n    num_samples=20,\n    range=[0, 1],\n    include_signal=True,\n    plot=False,\n)\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 2, 4\nfor i in range(1, cols * rows + 1):\n    M = i + 1\n    w = solve_for_w(xs=xs, ts=ts, M=M)\n    t_hats = predict(w, x_axis, M)\n    figure.add_subplot(rows, cols, i)\n    plt.axis(\"off\")\n    plt.title(f\"M = {M}\")\n    # Plot signal\n    plt.plot(x_axis, signal_values, label=\"Signal\", c=\"green\")\n    # Plot data\n    plt.scatter(xs, ts, s=3, label=\"Data\", c=\"black\")\n    # Plot predictions\n    plt.plot(x_axis, t_hats, label=\"Predicted Signal\")\nplt.show()\n\n\n\nSignal (green), training data (black), and fitted curves (blue) for various settings of M – the order of the polynomial."
  },
  {
    "objectID": "posts/curve_fitting/index.html#solving-for-the-parameters-with-a-penalty-term",
    "href": "posts/curve_fitting/index.html#solving-for-the-parameters-with-a-penalty-term",
    "title": "Approaches to Curve Fitting",
    "section": "2 Solving for the parameters with a penalty term",
    "text": "2 Solving for the parameters with a penalty term\n\n2.1 Analytical Setup\nWe have the error function \\(\\overset{\\sim}{E} (\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) \\}^2 + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2\\), where \\(\\lVert \\mathbf{w} \\rVert^2 = \\mathbf{w}^\\intercal \\mathbf{w} = w_0^2 + w_1^2 + \\dots + w_M^2\\) and the parameter \\(\\lambda\\) controls the strength of regularization. We’d like to find the optimal setting for \\(\\mathbf{w}\\) in the sense that it minimizes \\(\\overset{\\sim}{E} (\\mathbf{w})\\). We will see that we can cast this minimization problem as solving a system of linear equations. We will then solve that system of linear equations with code to attain the optimal \\(\\mathbf{w}^{*}\\).\nClaim: The \\(w_i\\) in \\(\\mathbf{w} = (w_1, w_2, ..., w_M)\\) that minimize the error function \\(\\overset{\\sim}{E} (\\mathbf{w})\\) are given by the solution to the following set of linear equations,\n\\[\n\\sum_{j=0}^M A_{ij}w_j + \\lambda w_i = T_i \\;\\; \\text{ where } \\;\\; A_{ij} = \\sum_{n=1}^N(x_n)^{(i+j)}, \\; T_i = \\sum_{n=1}^N (x_n)^i t_n\n\\]\nProof: We will take the derivative of \\(\\overset{\\sim}{E} (\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\), set it to zero, and then rearrange terms to prove the claim above.\nBy the chain rule,\n\\[\n\\frac{\\partial \\overset{\\sim}{E}(\\mathbf{w})}{\\partial w_i} = \\frac{\\partial E(\\mathbf{w})}{\\partial y(x_n, \\mathbf{w})} \\frac{\\partial y(x_n, \\mathbf{w})}{\\partial w_i} + \\frac{\\lambda}{2} \\frac{\\partial \\mathbf{w}^\\intercal \\mathbf{w}}{\\partial w_i} \\tag{2}\n\\]\nSolving the two terms on the right hand side yields\n\\[\n\\begin{aligned}\n&\\frac{\\partial \\overset{\\sim}{E}(\\mathbf{w})}{\\partial y(x_n, \\mathbf{w})} = \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\} \\\\\n&\\frac{\\partial y(x_n, \\mathbf{w})}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_0 + w_1x_1 + \\dots w_i x_n^i \\dots + w_m x_n^M) = x_n^i \\\\\n&\\frac{\\partial \\mathbf{w}^\\intercal \\mathbf{w}}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_0^2 + w_1^2 + \\dots w_i^2 + \\dots w_M^2) = 2 w_i\n\\end{aligned}\n\\]\nSubstituting back into \\((2)\\) yields\n\\[\n\\begin{aligned}\n\\frac{\\partial \\overset{\\sim}{E}(\\mathbf{w})}{\\partial w_i} & = \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n \\} x_n^i + \\frac{\\lambda}{2} 2 w_i \\\\\n& \\overset{(i)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{j} - t_n)x_n^i + \\lambda w_i  \\\\\n& \\overset{(ii)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{i}x_n^j - t_n x_n^i) + \\lambda w_i  \\\\\n& \\overset{(iii)}{=} \\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{(i+j)} - t_n x_n^i) + \\lambda w_i\n\\end{aligned}\n\\]\nwhere in \\((\\text{i})\\) we use the definition of \\(y(x_n, \\mathbf{w})\\) and \\(\\frac{\\lambda}{2} \\cdot 2 = \\lambda\\), in \\((\\text{ii})\\) we distribute \\(x_n^i\\) into the parentheses, and in \\((\\text{iii})\\) we use the exponent rule. Setting the derivative to \\(0\\) and rearranging,\n\\[\n\\begin{aligned}\n\\sum_{n=1}^N (\\sum_{j=0}^M w_j x_n^{(i+j)} - t_n x_n^i) + \\lambda w_i & = 0 \\\\\n\\sum_{n=1}^N \\sum_{j=0}^M w_j x_n^{(i+j)} + \\lambda w_i & = \\sum_{n=1}^N t_n x_n^i \\\\\n\\sum_{j=0}^M A_{ij}w_j + \\lambda w_i &= T_i\n\\end{aligned}\n\\]\n\\(\\blacksquare\\)\n\n\n2.2 Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Callable, List, Union, Tuple\n\n# ================ Generating Data ==================== #\ndef generate_data(\n    seed: int,\n    signal_fn: Callable,\n    noise: float,\n    num_samples: int,\n    range: List[float],\n    include_signal: bool = False,\n    plot: bool = False,\n) -&gt; Union[Tuple[Tuple[np.ndarray]], Tuple[np.ndarray]]:\n    \"\"\"\n    Generate noisy training data and the non-noisy data.\n\n    Parameters\n    ----------\n    seed : int\n        Seed for reproducibility.\n    signal_fn : Callable\n        A function describing the deterministic data generating process.\n    noise : float\n        The `scale` value for the Gaussian noise to be added.\n    num_samples : int\n        Number of training samples.\n    range : List[float]\n        Range of the input variable x.\n    plot : bool, optional\n        Whether to plot the data or not, by default False\n\n    Returns\n    -------\n    Union[Tuple[Tuple[np.ndarray]], Tuple[np.ndarray]]\n        If include_signal = True, two tuples where the first is the noisy\n        training data and the second is the underlying signal. If include_signal=False,\n        then a tuple with just the noisy training data.\n    \"\"\"\n    # Create generator for reproducible results\n    generator = np.random.default_rng(seed)\n    # Noisy signal\n    signal_noisy_fn = lambda xs: signal_fn(xs) + generator.normal(\n        loc=0, scale=noise, size=(num_samples,)\n    )\n    # Create training data\n    xs = np.linspace(start=range[0], stop=range[1], num=num_samples)\n    ts = signal_noisy_fn(xs)\n    if include_signal:\n        # Create \"signal\"\n        x_axis = np.linspace(start=0, stop=1, num=1000)\n        signal = signal_fn(x_axis)\n        to_return = ((xs, ts), (x_axis, signal))\n    else:\n        to_return = (xs, ts)\n    if plot:\n        # ===== Plot Data ====== #\n        # Create figure\n        figure1 = plt.figure(figsize=(8, 4))\n        plt.axis(\"off\")\n        # Plot Signal\n        line = np.linspace(start=0, stop=1, num=1000)\n        plt.plot(x_axis, signal, c=\"green\")\n        # Plot training data\n        plt.scatter(xs, ts, c=\"black\")\n        plt.show()\n    return to_return\n\n# ============= Solving for w ====================== #\ndef create_A(xs: np.array, M: int) -&gt; np.array:\n    \"\"\"\n    Create the matrix A where A_ij = sum_n=1^N [x_n^(i+j)].\n    \"\"\"\n    A = np.zeros((M, M))\n    for i in range(M):\n        for j in range(M):\n            A[i][j] = np.sum(xs ** (i + j))\n    return A\n\n\ndef create_T(xs: np.array, ts: np.array, M: int) -&gt; np.array:\n    \"\"\"\n    Create the vector T where T_i = sum_n=1^N (x_n^i) t_n.\n    \"\"\"\n    T = np.zeros((M,))\n    for i in range(M):\n        T[i] = np.sum((xs**i) * ts)\n    return T\n\n\ndef create_lambdaI(size: int, ln_lambda: float) -&gt; np.array:\n    \"\"\"\n    Create an identity matrix with lambda as the diagonal elements.\n    \"\"\"\n    lambda_ = np.exp(ln_lambda)\n    identity = np.eye(N=size)\n    return lambda_ * identity\n\n\ndef solve_for_w(\n    xs: np.array, ts: np.array, M: int, ln_lambda: float = None\n) -&gt; np.array:\n    \"\"\"\n    Creates A and T using `create_A` and `create_T` and then solves\n    the linear system of equations to get w.\n    \"\"\"\n    A = create_A(xs=xs, M=M)\n    T = create_T(xs=xs, ts=ts, M=M)\n    if ln_lambda:\n        lambdaI = create_lambdaI(size=M, ln_lambda=ln_lambda)\n        A_plus_lambdaI = A + lambdaI\n        return np.linalg.solve(a=A_plus_lambdaI, b=T)\n    else:\n        return np.linalg.solve(a=A, b=T)\n\n# ============== Prediction ====================== #\ndef predict(w: np.array, x: np.array, M) -&gt; np.array:\n    \"\"\"\n    Feature expand the input x and then run the linear\n    transformation involving w,x to get predictions for target t.\n    \"\"\"\n    # Get [x**1, x**2, x**3 ... x**M] for each x\n    N = x.shape[0]\n    powers = np.arange(0, M)\n    powers_expanded = np.tile(powers, (N,)).reshape(N, M)\n    xs_expanded = x.repeat(M).reshape(N, M)\n    xs_powered = np.power(xs_expanded, powers_expanded)\n    # Apply w\n    return w @ xs_powered.T\n\n# ============= Main ================== #\n(xs, ts), (x_axis, signal_values) = generate_data(\n    seed=123,\n    signal_fn=lambda x: np.sin(2 * np.pi * x),\n    noise=0.5,\n    num_samples=20,\n    range=[0, 1],\n    include_signal=False,\n    plot=True,\n)\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 2, 4\nfor i in range(1, cols * rows + 1):\n    M = i + 1\n    w = solve_for_w(xs=xs, ts=ts, M=M, ln_lambda=-8)\n    t_hats = predict(w, x_axis, M)\n    figure.add_subplot(rows, cols, i)\n    plt.axis(\"off\")\n    plt.title(f\"M = {M}\")\n    # Plot signal\n    plt.plot(x_axis, signal_values, label=\"Signal\", c=\"green\")\n    # Plot data\n    plt.scatter(xs, ts, s=3, label=\"Data\", c=\"black\")\n    # Plot predictions\n    plt.plot(x_axis, t_hats, label=\"Predicted Signal\")\nplt.show()\n\n\n\nSignal (green), training data (black), and fitted curves (blue) for various settings of M – the order of the polynomial, but with with regularization governed by \\(\\text{ln } \\lambda = 0\\)."
  },
  {
    "objectID": "posts/bayesian_experimental_design/index.html",
    "href": "posts/bayesian_experimental_design/index.html",
    "title": "Bayesian Experimental Design",
    "section": "",
    "text": "Preface\nThe goal of this post is to introduce the field of Bayesian experimental design. Bayesian experimental design contains popular subfields such as active learning and Bayesian optimization. I try and explan how these two subfields fit into the broader context of experimental design, and introduce notation as well as some important examples.\n\n\n1 Introduction\nThe context of experimental design is as follows: We have an unknown (hidden) process that we can sample from. We can think of “sampling from” as evaluating a function at some input points, or running an experiment for some configuration. We get to choose what to feed the black-box function (i.e. how to design the experiment) and this results in data (i.e. outcome of the experiment). We call the space of inputs that we can feed to the function (i.e. the space of configurations of an experiment) the design space, We want to intelligently choose what inputs (designs) to feed the function in order attain some goal. The reason we need to choose intelligently is that each “query” (i.e. evaluation of the function, i.e. experiment) is costly.\nBayesian experimental design is about intelligent probing of an unknown (black-box) function in order to elicit information that is useful for some goal we have in mind (Ivanova (2021)). What that goal is determines a particular sub-field of Bayesian experimental design.\n\nBayesian optimization: sample from the unknown function to optimize that function (Garrett (2023)). Then we have inputs that try and maximize this unknown function and we can elicit responses that we want by feeding the unknown function these inputs. For example, we want to find the hyper-parameters that result in the best validation accuracy. In this scenario, the design space is usually a finite set of inputs.\nActive learning: sample from the unknown function to learn that function. Then we have learned the unknown function to make predictions with; and if we’ve sampled smartly, we can learn a predictor efficiently. In this scenario, the design space is usually a pool of unlabelled data.\n\n\n\n2 Mathematical Notation\nWhile there are numerous ways to denote the aspects of Bayesian experimental design, we use a common one (see Ivanova (2021)).\n\nThe design of an experiment (i.e. the inputs): \\(\\delta\\)\nThe true underlying (random) process is a distribution over outcomes \\(y\\) given a design (input) \\(\\delta\\): \\(p^{*}(y|\\delta)\\)\nOur model of the underlying process: \\(p(y, \\theta |\\delta) = p(y|\\theta, \\delta)p(\\theta)\\).\n\nWhile the modeling itself (e.g. choosing a form for \\(p(y|\\theta, \\delta)\\) and \\(p(\\theta)\\)) is an important aspect of Bayesian experimental design, we usually treat them as given once they are chosen, and instead focus on choosing designs \\(\\delta\\) that bring us closer to our goals in the most efficient way possible.\n\n\n3 Choosing designs (inputs)\nThere are a few parts to choosing designs \\(\\delta\\) that bring us closer to our goals. The first is sometimes called building a policy (Garrett (2023)).\n\nA policy transforms our beliefs (captured in our model) into choosing designs (inputs) in order to bring us closer to our goals.\n\nA common approach to building a policy is to appeal to Bayesian decision theory, where we design a utility function that quantifies how useful a given set of experimental outcomes would be. The model summarizes our beliefs about experimental outcomes (i.e. what is the outcome of \\(p(y|\\theta, \\delta)\\) for a design \\(\\delta\\)?), and the policy summarizes our preferences (Garrett (2023)).\n\n\nReferences\n\n\nGarrett, Roman. 2023. “Bayesian Optimization.” https://www.youtube.com/watch?v=wZODGJzKmD0.\n\n\nIvanova, R. Desi. 2021. “Introduction to Bayesian Optimal Experimental Design.” https://desirivanova.com/post/boed-intro/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stuff",
    "section": "",
    "text": "Bayesian Experimental Design\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nApproaches to Curve Fitting\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]